# -*- coding: utf-8 -*-
"""thesis_model_confusion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/fadillarizalul/thesis-alzheimer/blob/main/thesis_model_confusion.ipynb
"""

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt

#import data dari drive
from google.colab import drive
drive.mount('/content/drive')

!wget --no-check-certificate \
https://github.com/fadillarizalul/thesis-alzheimer/raw/main/dataset/3-class-train-valid-test.zip \
  -O 3-class-train-valid-test.zip

!unzip /content/3-class-train-valid-test.zip

# re-size all the images to this
IMAGE_SIZE = [256, 256]

train_path="/content/3-class-train-valid-test/train"
test_path="/content/3-class-train-valid-test/test"
val_path="/content/3-class-train-valid-test/valid"

x_train=[]

for folder in os.listdir(train_path):
    sub_path=train_path+"/"+folder
    for img in os.listdir(sub_path):
        image_path=sub_path+"/"+img
        img_arr=cv2.imread(image_path)
        img_arr=cv2.resize(img_arr,(256,256))
        x_train.append(img_arr)

x_test=[]

for folder in os.listdir(test_path):
    sub_path=test_path+"/"+folder
    for img in os.listdir(sub_path):
        image_path=sub_path+"/"+img
        img_arr=cv2.imread(image_path)
        img_arr=cv2.resize(img_arr,(256,256))
        x_test.append(img_arr)

x_val=[]

for folder in os.listdir(val_path):
    sub_path=val_path+"/"+folder
    for img in os.listdir(sub_path):
        image_path=sub_path+"/"+img
        img_arr=cv2.imread(image_path)
        img_arr=cv2.resize(img_arr,(256,256))
        x_val.append(img_arr)

train_x=np.array(x_train)
test_x=np.array(x_test)
val_x=np.array(x_val)

train_x.shape,test_x.shape,val_x.shape

train_x=train_x/255.0
test_x=test_x/255.0
val_x=val_x/255.0

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# train_datagen = ImageDataGenerator(rescale = 1./255,
#                                    shear_range = 0.2,
#                                    zoom_range = 0.2,
#                                    horizontal_flip = True)

train_datagen = ImageDataGenerator(rescale = 1./255)
test_datagen = ImageDataGenerator(rescale = 1./255)
val_datagen = ImageDataGenerator(rescale = 1./255)


training_set = train_datagen.flow_from_directory(train_path,
                                                 target_size = (224, 224),
                                                 batch_size = 16,
                                                 class_mode = 'sparse')

test_set = test_datagen.flow_from_directory(test_path,
                                            target_size = (224, 224),
                                            batch_size = 16,
                                            class_mode = 'sparse')

val_set = val_datagen.flow_from_directory(val_path,
                                            target_size = (224, 224),
                                            batch_size = 16,
                                            class_mode = 'sparse')

training_set.class_indices

train_y=training_set.classes

test_y=test_set.classes

val_y=val_set.classes

train_y.shape,test_y.shape,val_y.shape

"""##vgg19"""

# add preprocessing layer to the front of VGG
vgg = VGG19(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)

# don't train existing weights
for layer in vgg.layers:
    layer.trainable = False

# our layers - you can add more if you want
x = Flatten()(vgg.output)

prediction = Dense(3, activation='softmax')(x)

# create a model object
modelvgg = Model(inputs=vgg.input, outputs=prediction)

# view the structure of the model
modelvgg.summary()

# tell the model what cost and optimization method to use
modelvgg.compile(
  loss='sparse_categorical_crossentropy',
  optimizer="adam",
  metrics=['accuracy']
)

from tensorflow.keras.callbacks import EarlyStopping
early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5)

#Early stopping to avoid overfitting of model

# fit the model
historyvgg = modelvgg.fit(
  train_x,
  train_y,
  validation_data=(val_x,val_y),
  epochs=25,
  #callbacks=[early_stop],
  batch_size=16,shuffle=True)

# loss
plt.plot(historyvgg.history['loss'], label='train loss')
plt.plot(historyvgg.history['val_loss'], label='val loss')
plt.legend()

plt.show()

# accuracies
plt.plot(historyvgg.history['accuracy'], label='train acc')
plt.plot(historyvgg.history['val_accuracy'], label='val acc')
plt.legend()

plt.show()

modelvgg.evaluate(test_x,test_y,batch_size=32)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
import numpy as np

y_predvgg=modelvgg.predict(test_x)
y_predvgg=np.argmax(y_predvgg,axis=1)

accuracy_score(y_predvgg,test_y)

print(classification_report(y_predvgg,test_y))

confusion_matrix(y_predvgg,test_y)

"""##resnet50"""

from tensorflow.keras.applications.resnet50 import ResNet50

base_model = ResNet50(weights='imagenet', 
                      include_top=False, 
                      input_shape=IMAGE_SIZE + [3])

base_model.summary()

base_model.trainable = False

import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.keras import layers, optimizers
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
import tensorflow.keras.backend as K

head = base_model.output

head = MaxPooling2D(pool_size=(4,4))(head)
head = Flatten(name='Flatten')(head)

# head = Dense(128, activation='elu')(head)
# head = Dropout(0.3)(head)

head = Dense(256, activation='elu')(head)
head = Dropout(0.3)(head)

head = Dense(3, activation='softmax')(head)

modelresnet = Model(base_model.input, head)

modelresnet.summary()

# tell the model what cost and optimization method to use
modelresnet.compile(
  loss='sparse_categorical_crossentropy',
  optimizer="adam",
  metrics=['accuracy']
)

# fit the model
historyresnet = modelresnet.fit(
  train_x,
  train_y,
  validation_data=(val_x,val_y),
  epochs=25,
  #callbacks=[early_stop],
  batch_size=16,shuffle=True)

# loss
plt.plot(historyresnet.history['loss'], label='train loss')
plt.plot(historyresnet.history['val_loss'], label='val loss')
plt.legend()

plt.show()

# accuracies
plt.plot(historyresnet.history['accuracy'], label='train acc')
plt.plot(historyresnet.history['val_accuracy'], label='val acc')
plt.legend()

plt.show()

modelresnet.evaluate(test_x,test_y,batch_size=32)

y_predresnet=modelresnet.predict(test_x)
y_predresnet=np.argmax(y_predresnet,axis=1)

accuracy_score(y_predresnet,test_y)

print(classification_report(y_predresnet,test_y))

confusion_matrix(y_predresnet,test_y)

"""#cnn 13"""

from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential
from keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout, BatchNormalization

model13 = keras.Sequential()

model13.add(layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation = 'relu', input_shape = (256, 256, 3), kernel_initializer="he_normal"))
model13.add(layers.MaxPooling2D())
model13.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same'))
model13.add(layers.MaxPooling2D())
model13.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same'))
model13.add(layers.MaxPooling2D())

model13.add(layers.Flatten())
model13.add(BatchNormalization())
model13.add(layers.Dense(units = 64, activation = 'relu'))
model13.add(layers.Dropout(0.3))
model13.add(layers.Dense(units = 64, activation = 'relu'))
model13.add(layers.Dense(units = 3, activation = 'softmax'))

model13.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])

model13.summary()

# tell the model what cost and optimization method to use
model13.compile(
  loss='sparse_categorical_crossentropy',
  optimizer="adam",
  metrics=['accuracy']
)

# fit the model
history13 = model13.fit(
  train_x,
  train_y,
  validation_data=(val_x,val_y),
  epochs=25,
  #callbacks=[early_stop],
  batch_size=16,shuffle=True)

# loss
plt.plot(history13.history['loss'], label='train loss')
plt.plot(history13.history['val_loss'], label='val loss')
plt.legend()

plt.show()

# accuracies
plt.plot(history13.history['accuracy'], label='train acc')
plt.plot(history13.history['val_accuracy'], label='val acc')
plt.legend()

plt.show()

model13.evaluate(test_x,test_y,batch_size=32)

y_pred13=model13.predict(test_x)
y_pred13=np.argmax(y_pred13,axis=1)

accuracy_score(y_pred13,test_y)

print(classification_report(y_pred13,test_y))

confusion_matrix(y_pred13,test_y)

"""# cnn 20"""

def build_model20():
    
    '''Sequential Model creation'''
    model = Sequential()
    
    model.add(Conv2D(16,(3,3),padding='same',input_shape = (256,256,3),activation='relu'))
    model.add(Conv2D(32,(3,3),padding='same',activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))
    
    model.add(Conv2D(32,(3,3),padding='same',activation='relu'))    
    model.add(Conv2D(64,(3,3),padding='same',activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))
    
    model.add(Conv2D(64,(3,3),padding='same',activation='relu'))
    model.add(Conv2D(128,(3,3),padding='same',activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))
    
    model.add(Conv2D(128,(3,3),padding='same',activation='relu'))
    model.add(Conv2D(128,(3,3),padding='same',activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding = 'same'))
    
    model.add(Flatten())
    model.add(Dense(32))
    model.add(Dropout(0.25))
    model.add(Dense(32))
    model.add(Dropout(0.25))
    model.add(Dense(32))
    model.add(Dense(units = 3, activation = 'softmax'))
    
    return model

model20 = build_model20()
model20.summary()

# tell the model what cost and optimization method to use
model20.compile(
  loss='sparse_categorical_crossentropy',
  optimizer="adam",
  metrics=['accuracy']
)

# fit the model
history20 = model20.fit(
  train_x,
  train_y,
  validation_data=(val_x,val_y),
  epochs=25,
  #callbacks=[early_stop],
  batch_size=16,shuffle=True)

# loss
plt.plot(history20.history['loss'], label='train loss')
plt.plot(history20.history['val_loss'], label='val loss')
plt.legend()

plt.show()

# accuracies
plt.plot(history20.history['accuracy'], label='train acc')
plt.plot(history20.history['val_accuracy'], label='val acc')
plt.legend()

plt.show()

model20.evaluate(test_x,test_y,batch_size=32)

y_pred20=model20.predict(test_x)
y_pred20=np.argmax(y_pred20,axis=1)

accuracy_score(y_pred20,test_y)

print(classification_report(y_pred20,test_y))

confusion_matrix(y_pred20,test_y)

